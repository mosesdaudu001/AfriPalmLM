# -*- coding: utf-8 -*-
"""AfriPalmLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aVs1mUkpXoybSawfMfAiqJFPEb27KdW7

## Installations
"""

!pip install datasets

"""## Get Data

"""

import os

from datasets import load_dataset

lang = ['afaanoromoo', 'amharic', 'gahuza', 'hausa', 'igbo', 'pidgin', 'somali', 'swahili', 'tigrinya', 'yoruba']

for lan in lang:
  train_dataset = load_dataset("castorini/afriberta-corpus", lan, split="train")
  test_dataset = load_dataset("castorini/afriberta-corpus", lan, split="test")

  # Extract the text data from the dataset
  train_data = train_dataset['text']
  test_data = test_dataset['text']


  train_folder = "data/train"
  test_folder = "data/eval"


  if not os.path.exists(train_folder):
      os.makedirs(train_folder)
      os.makedirs(test_folder)
      print('Folders created successfully!!')
  else:
      print("Directory already exists.")


  # Define the path to save the text file
  output_train_path = f"{train_folder}/train.{lan}"
  output_test_path = f"{test_folder}/eval.{lan}"

  # Write the text data to the text file
  with open(output_train_path, "w", encoding="utf-8") as file:
      for text in train_data:
          file.write(text + "\n")
  with open(output_test_path, "w", encoding="utf-8") as file:
      for text in test_data:
          file.write(text + "\n")

# import os


# def combine_dataset(typ):
#   # Directory containing the text files
#   directory = f"/content/data/{typ}"

#   # Output file name
#   output_file = f"/content/data/{typ}/all_{typ}.txt"

#   # Open the output file in write mode
#   with open(output_file, 'w') as outfile:
#       # Iterate over each file in the directory
#       for filename in os.listdir(directory):
#           # Open the file in read mode and read its content
#           with open(os.path.join(directory, filename), 'r') as infile:
#               content = infile.read()
#               # Write the content to the output file
#               outfile.write(content)
#               # Add a newline character after each file's content
#               outfile.write('\n')
#   print(f"length of {typ} dataset in characters: ", len(output_file))

# combine_dataset("train")
# combine_dataset("eval")

"""## Data processing"""

import os

dir = "/content/data/train"

total_vocab = 0
for file_name in os.listdir(dir):
  with open(os.path.join(dir, file_name), 'r') as infile:
      content = infile.read()

      chars = sorted(list(set(content)))
      vocab_size = len(chars)
      total_vocab = total_vocab + vocab_size
      # print(''.join(chars))

      print(f"For {file_name}: ")
      print("Vocab size is: ", vocab_size)
      print(f"Current token size is : {total_vocab}")
      print("---------------")
      del content

"""### Because we cannot load the entire dataset at once, let us get some random data from each language and join them together, so we can pass that as the corpus data to train the tokenizer

Also note that our dataset has a total of 12,392 separate tokens, which we will be adding to the PalmLM tokenizer. The original PalmLM tokenizer contains 256,000 unique tokens. But note that some of these tokens will be the same when we eventually combine the dataset, so the number will be much smaller. At the end of the day, we will be creating a tokenizer that should have 270,000 unique tokens (Based on our dataset)
"""

# comb_dir = "/content/combined_dataset/individual/train.afaanoromoo-v1"

# dr = comb_dir.rpartition("/")[0]
# dr = dr.rpartition("/")[0]
# print(f"{dr}/balable")

import nltk
from tqdm import tqdm
import unicodedata
import glob
import os
from random import sample

def sample_and_make_tempfile(sentences_dir, num_files):
    """ Use the set of files containing a sentence per line,
    sample num_files out of those and save as a temp file """

    sentence_files = glob.glob(sentences_dir + "/*.txt")

    # sample num_files
    sampled_files=sample(sentence_files, num_files)

    print("sampled files:")
    print(sampled_files)

    #read all the lines from sampled files and save to a list
    all_lines = []
    for filename in sampled_files:
        with open(filename) as f:
            lines = f.read().splitlines()

        all_lines.extend(lines)

    print("number of lines sampled:", len(all_lines))

    #combine into a single file and save
    data_dir = sentences_dir.rpartition("/")[0]
    data_dir = data_dir.rpartition("/")[0]
    data_dir = f"{data_dir}/combined"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    file_dir = sentences_dir.rpartition("/")[-1]
    tempfile_path = os.path.join(data_dir, f"{file_dir}-temp.txt")
    with open(tempfile_path, "w") as f:


        f.writelines(all_lines)
    print("Wrote to ", tempfile_path)
    return tempfile_path


# def chunks(sentences, n, tot_len):
#     """Yield successive n-sized chunks from sentences."""
#     for i in range(0, tot_len, n):
#         end_i = min(len(sentences),i + n)
#         yield sentences[i:end_i]

def get_training_corpus(data, chunksize):
    for start_idx in range(0, len(data), chunksize):
      samples = data[start_idx : start_idx+chunksize]
      yield samples

def make_sentence_files(dataset, data_dir = "jp_sentences"):
    """
    Make a sentence per line files, chuncsize sentences per file"""
    chunksize = 1000000
    # make sure data dir exists
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    # use simple regex for sentence tokenizing
    # sent_detector = nltk.RegexpTokenizer(u'[^　！？。]*[！？。.\n]')

    # loop over the chunks
    # for chunk_ind, sentence_chunk in enumerate(chunks(dataset, chunksize, len(dataset))):
    for chunk_ind, sentence_chunk in enumerate(get_training_corpus(dataset, chunksize)):

        # new file for each chunk
        filename = "sent_{}.txt".format(chunk_ind)
        filepath = os.path.join(data_dir, filename)

        print("writing to ", filepath)

        with open(filepath, "w") as f:

            lines = [
                line
                for line in sentence_chunk.splitlines()
                if (len(line.split()) > 5 and not line.isspace())
            ]


            f.writelines(lines)

# !rm -r "/content/train.tigrinya-v1"
# !rm -r "/content/combined_dataset"

dir = "/content/data/train"
comb_dir = "combined_dataset/individual"

for file_name in os.listdir(dir):
    filepath = os.path.join(dir, file_name)
    data_dir = f"{comb_dir}/{file_name}-v1"
    with open(filepath, 'r') as infile:
        content = infile.read()
        # print(len(content))
        make_sentence_files(content, data_dir = data_dir)

# dir = "/content/chunked_languages"
comb_dir = "combined_dataset/individual"

for file_name in os.listdir(comb_dir):
  file_path = os.path.join(comb_dir, file_name)
  # print(file_name)
  tempfile_path = sample_and_make_tempfile(
      sentences_dir = file_path,
      num_files = 5)

dir = "/content/combined_dataset/combined"

temp_path = "/content/combined_dataset/combined_temp.txt"

for file_name in os.listdir(dir):
  file_path = os.path.join(dir, file_name)
  with open(file_path, 'r') as infile:
      content = infile.read()
  with open(temp_path, "w") as f:
      f.writelines(content + '\n')
  print("Wrote to ", temp_path)
  # return tempfile_path

with open("/content/combined_dataset/combined_temp.txt", 'r') as f:
  text = f.read()

print(len(text))
print(len(sorted(list(set(text)))))

"""## Train Tokenizer"""

# def get_training_corpus(data, chunksize):
#     for start_idx in range(0, len(data), chunksize):
#       samples = data[start_idx : start_idx+chunksize]
#       yield samples

from transformers import AutoTokenizer

with open("/content/combined_dataset/combined_temp.txt", 'r') as f:
  text = f.read()

training_corpus = get_training_corpus(text, chunksize = 1000000)
old_tokenizer = AutoTokenizer.from_pretrained("DAMO-NLP-MT/polylm-1.7b")

new_tokenizer = old_tokenizer.train_new_from_iterator(text, 70000)
new_tokenizer.save_pretrained("PalmLM-70000-tokenizer")

"""## Test Tokenizers"""

!git clone https://github.com/mosesdaudu001/AfriPalmLM

from transformers import AutoTokenizer

tokenizer_path = "/content/AfriPalmLM/PalmLM-270000-tokenizer"
# tokenizer_path = "/content/PalmLM-70000-tokenizer"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

text = "Tarii warri gara uummata Oromootti deebi’anii"
print(tokenizer.tokenize(text))

text = "Tarii warri gara uummata Oromootti deebi’anii"
print(tokenizer.tokenize(text))

